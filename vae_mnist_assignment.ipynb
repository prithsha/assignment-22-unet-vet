{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from typing import List, Callable, Union, Any, TypeVar, Tuple\n",
    "import torchvision\n",
    "from torchvision.utils import save_image\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torch import tensor as Tensor\n",
    "Tensor = TypeVar('torch.tensor')\n",
    "manual_seed = 1265"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_dim = 400\n",
    "in_channels = 1\n",
    "latent_dim = 20\n",
    "batch_size = 128\n",
    "\n",
    "# kld_weight = 0.00025\n",
    "kld_weight = 0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VanillaVAE(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 in_channels: int,\n",
    "                 latent_dim: int,\n",
    "                 hidden_dims: List = None,\n",
    "                 **kwargs) -> None:\n",
    "        super(VanillaVAE, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "        out_channels = in_channels\n",
    "\n",
    "        modules = []\n",
    "        if hidden_dims is None:\n",
    "            hidden_dims = [32, 64, 128, 256, 512]\n",
    "\n",
    "        # Build Encoder\n",
    "        \n",
    "        for h_dim in hidden_dims:\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.Conv2d(in_channels, out_channels=h_dim,\n",
    "                              kernel_size= 3, stride= 2, padding  = 1),\n",
    "                    nn.BatchNorm2d(h_dim),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "            in_channels = h_dim\n",
    "\n",
    "        self.encoder = nn.Sequential(*modules)\n",
    "        self.fc_mu = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dims[-1], latent_dim)\n",
    "\n",
    "        # Build Decoder\n",
    "        modules = []\n",
    "        self.decoder_input = nn.Linear(latent_dim, hidden_dims[-1])\n",
    "        hidden_dims.reverse()\n",
    "\n",
    "        for i in range(len(hidden_dims) - 1):\n",
    "            print(f\"hidden_dims[i]: {hidden_dims[i]}, hidden_dims[i+1]: {hidden_dims[i+1]}\")\n",
    "            modules.append(\n",
    "                nn.Sequential(\n",
    "                    nn.ConvTranspose2d(hidden_dims[i],\n",
    "                                       hidden_dims[i + 1],\n",
    "                                       kernel_size=3,\n",
    "                                       stride = 2,\n",
    "                                       padding=1,\n",
    "                                       output_padding=1),\n",
    "                    nn.BatchNorm2d(hidden_dims[i + 1]),\n",
    "                    nn.LeakyReLU())\n",
    "            )\n",
    "\n",
    "\n",
    "\n",
    "        self.decoder = nn.Sequential(*modules)\n",
    "\n",
    "        self.final_layer = nn.Sequential(\n",
    "                            nn.ConvTranspose2d(hidden_dims[-1],\n",
    "                                               hidden_dims[-1],\n",
    "                                               kernel_size=3,\n",
    "                                               stride=2,\n",
    "                                               padding=2,\n",
    "                                               output_padding=1),\n",
    "                            nn.BatchNorm2d(hidden_dims[-1]),\n",
    "                            nn.LeakyReLU(),\n",
    "                            nn.Conv2d(hidden_dims[-1], out_channels= out_channels,\n",
    "                                      kernel_size= 3, padding= 0),\n",
    "                            nn.Tanh())\n",
    "\n",
    "    def encode(self, input: Tensor) -> List[Tensor]:\n",
    "        \"\"\"\n",
    "        Encodes the input by passing through the encoder network\n",
    "        and returns the latent codes.\n",
    "        :param input: (Tensor) Input tensor to encoder [N x C x H x W]\n",
    "        :return: (Tensor) List of latent codes\n",
    "        \"\"\"        \n",
    "        result = self.encoder(input)\n",
    "        # print(f\"encoder: {result.shape}\")  \n",
    "        result = torch.flatten(result, start_dim=1)\n",
    "\n",
    "        # Split the result into mu and var components\n",
    "        # of the latent Gaussian distribution\n",
    "        mu = self.fc_mu(result)\n",
    "        log_var = self.fc_var(result)\n",
    "\n",
    "        return [mu, log_var]\n",
    "\n",
    "    def decode(self, z: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Maps the given latent codes\n",
    "        onto the image space.\n",
    "        :param z: (Tensor) [B x D]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "        # print(f\"decoder shape of z: {z.shape}\")\n",
    "        result = self.decoder_input(z)\n",
    "\n",
    "        result = result.unsqueeze(-1).unsqueeze(-1)\n",
    "\n",
    "        # print(f\"shape of decoder_input output: {result.shape}\")\n",
    "        # result = result.view(-1, 512, 2, 2)\n",
    "        result = self.decoder(result)\n",
    "        # print(f\"shape of decoder output: {result.shape}\")\n",
    "        result = self.final_layer(result)\n",
    "        # print(f\"shape of final_layer output: {result.shape}\")\n",
    "        return result\n",
    "\n",
    "    def reparameterize(self, mu: Tensor, logvar: Tensor) -> Tensor:\n",
    "        \"\"\"\n",
    "        Reparameterization trick to sample from N(mu, var) from\n",
    "        N(0,1).\n",
    "        :param mu: (Tensor) Mean of the latent Gaussian [B x D]\n",
    "        :param logvar: (Tensor) Standard deviation of the latent Gaussian [B x D]\n",
    "        :return: (Tensor) [B x D]\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return eps * std + mu\n",
    "\n",
    "    def forward(self, input: Tensor, **kwargs) -> List[Tensor]:\n",
    "        mu, log_var = self.encode(input)\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        return  self.decode(z), mu, log_var\n",
    "\n",
    "    def sample(self,\n",
    "               num_samples:int,\n",
    "               current_device: int, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Samples from the latent space and return the corresponding\n",
    "        image space map.\n",
    "        :param num_samples: (Int) Number of samples\n",
    "        :param current_device: (Int) Device to run the model\n",
    "        :return: (Tensor)\n",
    "        \"\"\"\n",
    "        z = torch.randn(num_samples, self.latent_dim)\n",
    "\n",
    "        z = z.to(current_device)\n",
    "\n",
    "        samples = self.decode(z)\n",
    "        return samples\n",
    "\n",
    "    def generate(self, x: Tensor, **kwargs) -> Tensor:\n",
    "        \"\"\"\n",
    "        Given an input image x, returns the reconstructed image\n",
    "        :param x: (Tensor) [B x C x H x W]\n",
    "        :return: (Tensor) [B x C x H x W]\n",
    "        \"\"\"\n",
    "\n",
    "        return self.forward(x)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                           train=True,\n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='data',\n",
    "                                          train=False,\n",
    "                                          transform=transforms.ToTensor())\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hidden_dims[i]: 512, hidden_dims[i+1]: 256\n",
      "hidden_dims[i]: 256, hidden_dims[i+1]: 128\n",
      "hidden_dims[i]: 128, hidden_dims[i+1]: 64\n",
      "hidden_dims[i]: 64, hidden_dims[i+1]: 32\n"
     ]
    }
   ],
   "source": [
    "model = VanillaVAE(in_channels=1, latent_dim=latent_dim).to(device)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(recons,input, mu, log_var) -> dict:\n",
    "    \"\"\"\n",
    "    Computes the VAE loss function.\n",
    "    KL(N(\\mu, \\sigma), N(0, 1)) = \\log \\frac{1}{\\sigma} + \\frac{\\sigma^2 + \\mu^2}{2} - \\frac{1}{2}\n",
    "    :param args:\n",
    "    :param kwargs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "\n",
    "    recons_loss =F.mse_loss(recons, input)\n",
    "    kld_loss = torch.mean(-0.5 * torch.sum(1 + log_var - mu ** 2 - log_var.exp(), dim = 1), dim = 0)\n",
    "\n",
    "    loss = recons_loss + kld_weight * kld_loss\n",
    "\n",
    "    # print ({'loss': loss, 'Reconstruction_Loss':recons_loss.detach(), 'KLD':-kld_loss.detach()})\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train function\n",
    "def train(epoch):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for i, (images, _) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        reconstructed, mu, logvar = model(images)\n",
    "        # print(f\"model output reconstructed.shape: {reconstructed.shape}\")\n",
    "        # print(f\"input images shape output: {images.shape}\")\n",
    "        loss = loss_function(reconstructed, images, mu, logvar)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if i % 100 == 0:\n",
    "            print(\"Train Epoch {} [Batch {}/{}]\\tLoss: {:.4f}\".format(epoch, i, len(train_loader), loss.item()/len(images)))\n",
    "            \n",
    "    print('=====> Epoch {}, Average Loss: {:.4f}'.format(epoch, train_loss/len(train_loader.dataset)))\n",
    "\n",
    "\n",
    "# Test function\n",
    "def test(epoch):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (images, _) in enumerate(test_loader):\n",
    "            images = images.to(device)\n",
    "            reconstructed, mu, logvar = model(images)\n",
    "            test_loss += loss_function(reconstructed, images, mu, logvar)\n",
    "            if batch_idx == 0:\n",
    "                comparison = torch.cat([images[:5], reconstructed.view(batch_size, 1, 28, 28)[:5]])\n",
    "                save_image(comparison.cpu(), 'results/reconstruction_' + str(epoch) + '.png', nrow = 5)\n",
    "\n",
    "    print('=====> Average Test Loss: {:.4f}'.format(test_loss/len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch 1 [Batch 0/469]\tLoss: 0.0018\n",
      "Train Epoch 1 [Batch 100/469]\tLoss: 0.0003\n",
      "Train Epoch 1 [Batch 200/469]\tLoss: 0.0002\n",
      "Train Epoch 1 [Batch 300/469]\tLoss: 0.0001\n",
      "Train Epoch 1 [Batch 400/469]\tLoss: 0.0001\n",
      "=====> Epoch 1, Average Loss: 0.0002\n",
      "=====> Average Test Loss: 0.0001\n",
      "Train Epoch 2 [Batch 0/469]\tLoss: 0.0001\n",
      "Train Epoch 2 [Batch 100/469]\tLoss: 0.0001\n",
      "Train Epoch 2 [Batch 200/469]\tLoss: 0.0001\n",
      "Train Epoch 2 [Batch 300/469]\tLoss: 0.0001\n",
      "Train Epoch 2 [Batch 400/469]\tLoss: 0.0001\n",
      "=====> Epoch 2, Average Loss: 0.0001\n",
      "=====> Average Test Loss: 0.0001\n"
     ]
    }
   ],
   "source": [
    "# Main function\n",
    "\n",
    "epochs = 20\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(epoch)\n",
    "    test(epoch)\n",
    "    with torch.no_grad():\n",
    "        # Get rid of the encoder and sample z from the gaussian ditribution and feed it to the decoder to generate samples\n",
    "        sample = torch.randn(64,20).to(device)\n",
    "        generated = model.decode(sample).cpu()\n",
    "        save_image(generated.view(64,1,28,28), 'results/x_sample_' + str(epoch) + '.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
